{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "\n",
    "# set up environment\n",
    "openai = OpenAI()\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "156d283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt(selected_model):\n",
    "    return f\"\"\"\n",
    "    You are a tech expert and know every coding language, and can give \n",
    "    nice, detailed and simple explanations for the given questions.\n",
    "    Introduce yourself by saying which model you are every time you answer. For example, this is {selected_model}.  \n",
    "    \"\"\"\n",
    "\n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"gpt-4o-mini-tts\",\n",
    "        voice=\"onyx\",\n",
    "        input=message)\n",
    "\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    output_filename = \"output_audio.mp3\"\n",
    "    with open(output_filename, \"wb\") as f:\n",
    "        f.write(audio_stream.read())\n",
    "\n",
    "    display(Audio(output_filename, autoplay=True))\n",
    "\n",
    "def listener(audio_file):\n",
    "    with open(audio_file, \"rb\") as audio:\n",
    "        transcript = openai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio\n",
    "        )\n",
    "    return transcript.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ff8eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history, selected_model):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt(selected_model)}] + history\n",
    "\n",
    "    if selected_model == \"GPT-4o-mini\":\n",
    "        stream = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, stream=True)\n",
    "        response = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            try:\n",
    "                response += chunk.choices[0].delta.content or ''\n",
    "                updated_history = history + [{\"role\": \"assistant\", \"content\": response}]\n",
    "                # talker(response)\n",
    "                yield updated_history  \n",
    "            except Exception as e:\n",
    "                print(f\"Streaming error: {e}\")\n",
    "                yield \"Sorry, there was an error processing your request.\"\n",
    "    elif selected_model == \"Llama3.2\":\n",
    "        stream = ollama.chat.completions.create(model=\"llama3.2\", messages=messages, stream=True)        \n",
    "        response = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            try:\n",
    "                response += chunk.choices[0].delta.content or ''\n",
    "                updated_history = history + [{\"role\": \"assistant\", \"content\": response}]\n",
    "                # talker(response)\n",
    "                yield updated_history  \n",
    "            except Exception as e:\n",
    "                print(f\"Streaming error: {e}\")\n",
    "                yield \"Sorry, there was an error processing your request.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "\n",
    "    gr.Markdown(\"## AI Code Assistant\")\n",
    "    gr.Markdown(\"**Select your preferred AI model:**\")\n",
    "    \n",
    "    model_dropdown = gr.Dropdown(\n",
    "        choices=[\"GPT-4o-mini\", \"Llama3.2\"], \n",
    "        value=\"GPT-4o-mini\",  # default selection\n",
    "        label=\"Choose Model\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=200, type=\"messages\")\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        # Audio input for voice messages\n",
    "        audio_input = gr.Audio(\n",
    "            sources=[\"microphone\", \"upload\"], \n",
    "            type=\"filepath\", \n",
    "            label=\"üéôÔ∏è Voice Message\"\n",
    "        )\n",
    "    with gr.Row():\n",
    "        voice_submit = gr.Button(\"Send Voice Message\", variant=\"secondary\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        return \"\", history\n",
    "\n",
    "    def process_voice_input(audio_file):\n",
    "        \"\"\"Convert voice to text and put it in the text box\"\"\"\n",
    "        if audio_file is not None:\n",
    "            transcribed_text = listener(audio_file)\n",
    "            if transcribed_text and not transcribed_text.startswith(\"Error\"):\n",
    "                return transcribed_text\n",
    "        return \"\"\n",
    "    \n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=[chatbot, model_dropdown], outputs=[chatbot]\n",
    "    )\n",
    "\n",
    "    voice_submit.click(\n",
    "        process_voice_input,\n",
    "        inputs=[audio_input],\n",
    "        outputs=[entry]\n",
    "    )\n",
    "\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cda2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
